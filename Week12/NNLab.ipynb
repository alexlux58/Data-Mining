{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NNLab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "id": "-7m-SFx_mq49",
        "outputId": "eb9ba6c5-ecc8-488d-c50d-0bb03f1e5b7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f6de6dcd290>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAECCAYAAAD+eGJTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAN2klEQVR4nO3dX4wd9XnG8efBXhtsQLEhuA4YnBJ8kVaqqRaowp+6IkU0SmVQIqtISV0pqrmIpSDlAmq1giqqSqIAidrKksFuHIkQEQHFFyQFLFSKGjksxMIG05JSu9gxa1MnsgnG2N63Fzu0G9j9nd09c2bO7vv9SGjPznvOnicTeDxz5udZR4QA5HVG2wEAtIsSAJKjBIDkKAEgOUoASI4SAJJrpQRs32j7323/zPYdbWQosb3X9i7bO20P9UGeLbYP2d49Ztti20/Zfq36uqjP8t1l+0C1D3fa/kyL+ZbZfsb2K7Zftv2Vantf7MNCvkb2oZteJ2B7jqT/kPSHkvZLel7SLRHxSqNBCmzvlTQYEW+1nUWSbF8n6W1J342I3662fUPSkYi4uyrSRRFxex/lu0vS2xHxzTYyjWV7qaSlEfGi7XMkvSDpJkl/pj7Yh4V8a9TAPmzjSOBKST+LiNcj4j1J35e0uoUcM0ZEPCvpyAc2r5a0tXq8VaP/0rRignx9IyIORsSL1eNjkvZIulB9sg8L+RrRRglcKOmNMd/vV4P/gycpJD1p+wXb69oOM4ElEXGwevympCVthpnAetsvVacLrZ2ujGV7uaTLJe1QH+7DD+STGtiHfDA4vmsi4ncl/ZGkL1eHu30rRs/p+m3990ZJl0paKemgpHvajSPZPlvSI5Jui4ijY2f9sA/HydfIPmyjBA5IWjbm+4uqbX0jIg5UXw9JekyjpzD9Zrg6l3z/nPJQy3l+TUQMR8TpiBiRdL9a3oe2BzT6H9iDEfFotblv9uF4+Zrah22UwPOSLrP9cdvzJP2JpG0t5BiX7YXVhzOyvVDSDZJ2l1/Vim2S1laP10p6vMUsH/L+f1yVm9XiPrRtSZsl7YmIe8eM+mIfTpSvqX3Y+NUBSaoudXxL0hxJWyLibxoPMQHbv6nRP/0laa6k77Wdz/ZDklZJOl/SsKQ7Jf2TpIclXSxpn6Q1EdHKh3MT5Ful0cPYkLRX0q1jzr+bzneNpH+VtEvSSLV5g0bPu1vfh4V8t6iBfdhKCQDoH3wwCCRHCQDJUQJAcpQAkBwlACTXagn08ZJcSeTrVj/n6+dsUrP52j4S6Ov/I0S+bvVzvn7OJjWYr+0SANCyrhYL2b5R0rc1uvLvgYi4u/T8eZ4fZ2rh/31/Uic0oPnTfv9eI193+jlfP2eT6s/3rn6l9+KEx5tNuwSmc3OQc704rvL103o/ANO3I7braBwZtwS6OR3g5iDALNBNCcyEm4MA6GBur9+gutSxTpLO1IJevx2AKermSGBSNweJiE0RMRgRg/38QQyQVTcl0Nc3BwEwOdM+HYiIU7bXS/pn/f/NQV6uLRmARnT1mUBEPCHpiZqyAGgBKwaB5CgBIDlKAEiOEgCSowSA5CgBIDlKAEiOEgCSowSA5CgBIDlKAEiOEgCSowSA5CgBIDlKAEiOEgCSowSA5CgBIDlKAEiOEgCSowSA5CgBIDlKAEiOEgCSowSA5CgBIDlKAEiOEgCSowSA5CgBILmufjU5MJP86vNXFedf/8bG4vxra/60OI+h3VPO1A+6KgHbeyUdk3Ra0qmIGKwjFIDm1HEk8AcR8VYNPwdAC/hMAEiu2xIISU/afsH2ujoCAWhWt6cD10TEAdsXSHrK9qsR8ezYJ1TlsE6SztSCLt8OQN26OhKIiAPV10OSHpN05TjP2RQRgxExOKD53bwdgB6YdgnYXmj7nPcfS7pB0sy8RgIk1s3pwBJJj9l+/+d8LyJ+VEuqHjm++kMHKr8+P29Ocb54y4/rjIOGHRos/5n3tb1/3FCS/jLtEoiI1yX9To1ZALSAS4RAcpQAkBwlACRHCQDJUQJAcpQAkFyq+wn8/Lpy5y249JflH7ClxjCo3xnldR5x8fHi/PoLXi3Ot/tTU440E3AkACRHCQDJUQJAcpQAkBwlACRHCQDJUQJAcqnWCfz1Z39QnH99zw0NJUEvzLn0kuL81d8vL/RY+ZMvFOcfe37XlDPNBBwJAMlRAkBylACQHCUAJEcJAMlRAkBylACQXKp1AgM+1XYE9NDcB97p6vXH//PcmpLMLBwJAMlRAkBylACQHCUAJEcJAMlRAkBylACQ3KxaJzByzcri/Nozn2soCdqwfOH/dPX6ZU+frinJzNLxSMD2FtuHbO8es22x7adsv1Z9XdTbmAB6ZTKnA9+RdOMHtt0haXtEXCZpe/U9gBmoYwlExLOSjnxg82pJW6vHWyXdVHMuAA2Z7geDSyLiYPX4TUlLasoDoGFdXx2IiJAUE81tr7M9ZHvopE50+3YAajbdEhi2vVSSqq+HJnpiRGyKiMGIGBzQ/Gm+HYBemW4JbJO0tnq8VtLj9cQB0LSO6wRsPyRplaTzbe+XdKekuyU9bPtLkvZJWtPLkJO177NnFecXzFnQUBL0wtzlFxfnn1+8rauff9Z//aI4n62rCDqWQETcMsHo+pqzAGgBy4aB5CgBIDlKAEiOEgCSowSA5CgBILlZdT+BuZ841tXr3331IzUlQS+88a2FxfnV80eK881HLyq/wS+PTjXSrMCRAJAcJQAkRwkAyVECQHKUAJAcJQAkRwkAyc2qdQLdumCofJ0ZZXPOP684H/7ciuJ88Zr9xfm/rNjcIcGZxenGfyjfD/eC4X/r8PNnJ44EgOQoASA5SgBIjhIAkqMEgOQoASA5SgBIjnUCYxxfXO7E8t9m797ItZcX5zHHxfkbny7/hqf3PnayOD9jXvnO+k9e+3fF+UA5nt48Xc73V6/fXJwfGSmv41hwRjn/kh3l+01M+Lv0ZjmOBIDkKAEgOUoASI4SAJKjBIDkKAEgOUoASG5WrRM48e5AcT7S4UrwP264rzjftn7llDNNxe3nPVCcn6Hyhfjj8V5x/vPT5evof394VXH+6advK84/8tN5xfnSJ4eLc+8r30/g8J6zivMlc8rrIOL5XcV5Vh2PBGxvsX3I9u4x2+6yfcD2zuqfz/Q2JoBemczpwHck3TjO9vsiYmX1zxP1xgLQlI4lEBHPSjrSQBYALejmg8H1tl+qThcW1ZYIQKOmWwIbJV0qaaWkg5LumeiJttfZHrI9dFInpvl2AHplWiUQEcMRcToiRiTdL+nKwnM3RcRgRAwOqPy3yAA0b1olYHvpmG9vlrR7oucC6G8d1wnYfkjSKknn294v6U5Jq2yv1Ohfwd4r6dYeZpy0T3zhp8X5b/3t+uJ82RUH6owzZc8cKt+X//APLyrOz3u5fJ183o+e75Cg/PoVGurw+rLyKgXpwO2fKs6vmP/j4vz7b184xUSQJlECEXHLOJs7/RYIADMEy4aB5CgBIDlKAEiOEgCSowSA5CgBILlZdT+BTj7+F+XrzP1uqf677Qg9teC6w129/i+f+VxxvkI/6ernz1YcCQDJUQJAcpQAkBwlACRHCQDJUQJAcpQAkFyqdQKY3S55vPx7JTA+jgSA5CgBIDlKAEiOEgCSowSA5CgBIDlKAEiOEgCSowSA5CgBIDlKAEiOEgCSowSA5CgBIDlKAEiO+wlgxpjj8p9Zv1gxUJz/xg/rTDN7dDwSsL3M9jO2X7H9su2vVNsX237K9mvV10W9jwugbpM5HTgl6asR8UlJvyfpy7Y/KekOSdsj4jJJ26vvAcwwHUsgIg5GxIvV42OS9ki6UNJqSVurp22VdFOvQgLonSl9MGh7uaTLJe2QtCQiDlajNyUtqTUZgEZMugRsny3pEUm3RcTRsbOICEnj3uXR9jrbQ7aHTupEV2EB1G9SJWB7QKMF8GBEPFptHra9tJovlXRovNdGxKaIGIyIwQHNryMzgBpN5uqAJW2WtCci7h0z2iZpbfV4raTH648HoNcms07gaklflLTL9s5q2wZJd0t62PaXJO2TtKY3EYFRp2Ok/ASWvk1LxxKIiOckeYLx9fXGAdA0uhNIjhIAkqMEgOQoASA5SgBIjhIAkuN+Apg13rninbYjzEgcCQDJUQJAcpQAkBwlACRHCQDJUQJAcpQAkBzrBDBjdPq9A5ge9iqQHCUAJEcJAMlRAkBylACQHCUAJEcJAMmxTgB948TTHy3OT6/s8HsHMC0cCQDJUQJAcpQAkBwlACRHCQDJUQJAcpQAkJwjovwEe5mk70paIikkbYqIb9u+S9KfSzpcPXVDRDxR+lnnenFcZX6bOdC0HbFdR+OIx5tNZrHQKUlfjYgXbZ8j6QXbT1Wz+yLim3UFBdC8jiUQEQclHaweH7O9R9KFvQ4GoBlT+kzA9nJJl0vaUW1ab/sl21tsL6o5G4AGTLoEbJ8t6RFJt0XEUUkbJV0qaaVGjxTumeB162wP2R46qRM1RAZQp0mVgO0BjRbAgxHxqCRFxHBEnI6IEUn3S7pyvNdGxKaIGIyIwQHNrys3gJp0LAHblrRZ0p6IuHfM9qVjnnazpN31xwPQa5O5OnC1pC9K2mV7Z7Vtg6RbbK/U6GXDvZJu7UlCAD01masDz0ka7/picU0AgJmBFYNAcpQAkBwlACRHCQDJUQJAcpQAkBwlACRHCQDJUQJAcpQAkBwlACRHCQDJUQJAcpQAkBwlACTX8fcO1Ppm9mFJ+8ZsOl/SW40FmDrydaef8/VzNqn+fJdExEfHGzRaAh96c3soIgZbC9AB+brTz/n6OZvUbD5OB4DkKAEgubZLYFPL798J+brTz/n6OZvUYL5WPxMA0L62jwQAtIwSAJKjBIDkKAEgOUoASO5/AYwHwFX7l8yWAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "print(x_train.shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.matshow(x_train[2])\n",
        "plt.matshow(x_test[2])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "x_train = np.reshape(x_train, (60000,784))\n",
        "x_test = np.reshape(x_test, (10000,784))\n",
        "clf = MLPClassifier(hidden_layer_sizes=(10,5,), activation=\"relu\", verbose=True, solver=\"adam\").fit(x_train, y_train)\n",
        "pred = clf.predict(x_test)\n",
        "print(classification_report(y_test, pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4py_-ORoa0h",
        "outputId": "bc932039-2ec1-4ca7-ae04-5b4e0723dd24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.75107744\n",
            "Iteration 2, loss = 2.13061975\n",
            "Iteration 3, loss = 2.01473109\n",
            "Iteration 4, loss = 1.96907807\n",
            "Iteration 5, loss = 1.94282553\n",
            "Iteration 6, loss = 1.92294387\n",
            "Iteration 7, loss = 1.90663756\n",
            "Iteration 8, loss = 1.88460618\n",
            "Iteration 9, loss = 1.85029859\n",
            "Iteration 10, loss = 1.82163324\n",
            "Iteration 11, loss = 1.80026629\n",
            "Iteration 12, loss = 1.76648482\n",
            "Iteration 13, loss = 1.46958592\n",
            "Iteration 14, loss = 1.31348953\n",
            "Iteration 15, loss = 1.20286617\n",
            "Iteration 16, loss = 1.07144732\n",
            "Iteration 17, loss = 0.92759783\n",
            "Iteration 18, loss = 0.74762668\n",
            "Iteration 19, loss = 0.66118934\n",
            "Iteration 20, loss = 0.62588349\n",
            "Iteration 21, loss = 0.60280438\n",
            "Iteration 22, loss = 0.58169159\n",
            "Iteration 23, loss = 0.56672803\n",
            "Iteration 24, loss = 0.54857966\n",
            "Iteration 25, loss = 0.53951324\n",
            "Iteration 26, loss = 0.52696069\n",
            "Iteration 27, loss = 0.51807793\n",
            "Iteration 28, loss = 0.51035864\n",
            "Iteration 29, loss = 0.50686983\n",
            "Iteration 30, loss = 0.50017507\n",
            "Iteration 31, loss = 0.49427367\n",
            "Iteration 32, loss = 0.48768452\n",
            "Iteration 33, loss = 0.48485833\n",
            "Iteration 34, loss = 0.48154018\n",
            "Iteration 35, loss = 0.47610942\n",
            "Iteration 36, loss = 0.47264253\n",
            "Iteration 37, loss = 0.47189879\n",
            "Iteration 38, loss = 0.46706398\n",
            "Iteration 39, loss = 0.46392763\n",
            "Iteration 40, loss = 0.46473762\n",
            "Iteration 41, loss = 0.46141661\n",
            "Iteration 42, loss = 0.46043882\n",
            "Iteration 43, loss = 0.45691257\n",
            "Iteration 44, loss = 0.45703898\n",
            "Iteration 45, loss = 0.45457875\n",
            "Iteration 46, loss = 0.45468474\n",
            "Iteration 47, loss = 0.45388796\n",
            "Iteration 48, loss = 0.45361810\n",
            "Iteration 49, loss = 0.45072709\n",
            "Iteration 50, loss = 0.44888164\n",
            "Iteration 51, loss = 0.44751331\n",
            "Iteration 52, loss = 0.44684607\n",
            "Iteration 53, loss = 0.44763481\n",
            "Iteration 54, loss = 0.44545830\n",
            "Iteration 55, loss = 0.44408283\n",
            "Iteration 56, loss = 0.44336665\n",
            "Iteration 57, loss = 0.44251207\n",
            "Iteration 58, loss = 0.44249497\n",
            "Iteration 59, loss = 0.44314598\n",
            "Iteration 60, loss = 0.44551025\n",
            "Iteration 61, loss = 0.44150105\n",
            "Iteration 62, loss = 0.44055803\n",
            "Iteration 63, loss = 0.44016761\n",
            "Iteration 64, loss = 0.44055382\n",
            "Iteration 65, loss = 0.43697886\n",
            "Iteration 66, loss = 0.43820298\n",
            "Iteration 67, loss = 0.43686001\n",
            "Iteration 68, loss = 0.43459240\n",
            "Iteration 69, loss = 0.43464584\n",
            "Iteration 70, loss = 0.43922192\n",
            "Iteration 71, loss = 0.43331055\n",
            "Iteration 72, loss = 0.43211163\n",
            "Iteration 73, loss = 0.43089594\n",
            "Iteration 74, loss = 0.42752690\n",
            "Iteration 75, loss = 0.42935067\n",
            "Iteration 76, loss = 0.43005080\n",
            "Iteration 77, loss = 0.42809289\n",
            "Iteration 78, loss = 0.43050457\n",
            "Iteration 79, loss = 0.42583239\n",
            "Iteration 80, loss = 0.42410198\n",
            "Iteration 81, loss = 0.42690709\n",
            "Iteration 82, loss = 0.42360759\n",
            "Iteration 83, loss = 0.42531751\n",
            "Iteration 84, loss = 0.42167082\n",
            "Iteration 85, loss = 0.42416107\n",
            "Iteration 86, loss = 0.42011642\n",
            "Iteration 87, loss = 0.41895212\n",
            "Iteration 88, loss = 0.42416763\n",
            "Iteration 89, loss = 0.41939584\n",
            "Iteration 90, loss = 0.42082071\n",
            "Iteration 91, loss = 0.41959921\n",
            "Iteration 92, loss = 0.41860297\n",
            "Iteration 93, loss = 0.41984485\n",
            "Iteration 94, loss = 0.41657128\n",
            "Iteration 95, loss = 0.41558806\n",
            "Iteration 96, loss = 0.41361786\n",
            "Iteration 97, loss = 0.41226783\n",
            "Iteration 98, loss = 0.41120268\n",
            "Iteration 99, loss = 0.41130014\n",
            "Iteration 100, loss = 0.40838564\n",
            "Iteration 101, loss = 0.41186679\n",
            "Iteration 102, loss = 0.40708692\n",
            "Iteration 103, loss = 0.40732832\n",
            "Iteration 104, loss = 0.40375207\n",
            "Iteration 105, loss = 0.40299541\n",
            "Iteration 106, loss = 0.40232256\n",
            "Iteration 107, loss = 0.40147885\n",
            "Iteration 108, loss = 0.40106656\n",
            "Iteration 109, loss = 0.39961617\n",
            "Iteration 110, loss = 0.39986843\n",
            "Iteration 111, loss = 0.39764007\n",
            "Iteration 112, loss = 0.39666344\n",
            "Iteration 113, loss = 0.39674021\n",
            "Iteration 114, loss = 0.39292177\n",
            "Iteration 115, loss = 0.39157433\n",
            "Iteration 116, loss = 0.39037449\n",
            "Iteration 117, loss = 0.38915334\n",
            "Iteration 118, loss = 0.38871355\n",
            "Iteration 119, loss = 0.38504132\n",
            "Iteration 120, loss = 0.38580275\n",
            "Iteration 121, loss = 0.38401224\n",
            "Iteration 122, loss = 0.38539143\n",
            "Iteration 123, loss = 0.38031137\n",
            "Iteration 124, loss = 0.38226244\n",
            "Iteration 125, loss = 0.38198759\n",
            "Iteration 126, loss = 0.37827943\n",
            "Iteration 127, loss = 0.37706594\n",
            "Iteration 128, loss = 0.37800951\n",
            "Iteration 129, loss = 0.37800955\n",
            "Iteration 130, loss = 0.37692240\n",
            "Iteration 131, loss = 0.37542580\n",
            "Iteration 132, loss = 0.37549630\n",
            "Iteration 133, loss = 0.37533204\n",
            "Iteration 134, loss = 0.37353946\n",
            "Iteration 135, loss = 0.37471065\n",
            "Iteration 136, loss = 0.37628845\n",
            "Iteration 137, loss = 0.37394985\n",
            "Iteration 138, loss = 0.37476842\n",
            "Iteration 139, loss = 0.37296781\n",
            "Iteration 140, loss = 0.37193649\n",
            "Iteration 141, loss = 0.37343619\n",
            "Iteration 142, loss = 0.36938646\n",
            "Iteration 143, loss = 0.37164507\n",
            "Iteration 144, loss = 0.37050670\n",
            "Iteration 145, loss = 0.37088714\n",
            "Iteration 146, loss = 0.36940235\n",
            "Iteration 147, loss = 0.37014984\n",
            "Iteration 148, loss = 0.36985457\n",
            "Iteration 149, loss = 0.37065562\n",
            "Iteration 150, loss = 0.36850789\n",
            "Iteration 151, loss = 0.36655176\n",
            "Iteration 152, loss = 0.36625340\n",
            "Iteration 153, loss = 0.36779194\n",
            "Iteration 154, loss = 0.36583770\n",
            "Iteration 155, loss = 0.36726323\n",
            "Iteration 156, loss = 0.36593901\n",
            "Iteration 157, loss = 0.36656367\n",
            "Iteration 158, loss = 0.36814280\n",
            "Iteration 159, loss = 0.36728098\n",
            "Iteration 160, loss = 0.36507902\n",
            "Iteration 161, loss = 0.36707831\n",
            "Iteration 162, loss = 0.36499331\n",
            "Iteration 163, loss = 0.36722549\n",
            "Iteration 164, loss = 0.36592718\n",
            "Iteration 165, loss = 0.36604908\n",
            "Iteration 166, loss = 0.36422899\n",
            "Iteration 167, loss = 0.36512721\n",
            "Iteration 168, loss = 0.36263514\n",
            "Iteration 169, loss = 0.36537324\n",
            "Iteration 170, loss = 0.36451514\n",
            "Iteration 171, loss = 0.36524586\n",
            "Iteration 172, loss = 0.36351686\n",
            "Iteration 173, loss = 0.36244537\n",
            "Iteration 174, loss = 0.36219671\n",
            "Iteration 175, loss = 0.36329529\n",
            "Iteration 176, loss = 0.36244916\n",
            "Iteration 177, loss = 0.36249987\n",
            "Iteration 178, loss = 0.36491706\n",
            "Iteration 179, loss = 0.36368031\n",
            "Iteration 180, loss = 0.36350810\n",
            "Iteration 181, loss = 0.36122553\n",
            "Iteration 182, loss = 0.36399003\n",
            "Iteration 183, loss = 0.36317867\n",
            "Iteration 184, loss = 0.36216972\n",
            "Iteration 185, loss = 0.36044280\n",
            "Iteration 186, loss = 0.36011237\n",
            "Iteration 187, loss = 0.36219575\n",
            "Iteration 188, loss = 0.36435763\n",
            "Iteration 189, loss = 0.35982583\n",
            "Iteration 190, loss = 0.36146534\n",
            "Iteration 191, loss = 0.35946874\n",
            "Iteration 192, loss = 0.36378219\n",
            "Iteration 193, loss = 0.36008340\n",
            "Iteration 194, loss = 0.36076773\n",
            "Iteration 195, loss = 0.36206776\n",
            "Iteration 196, loss = 0.35958163\n",
            "Iteration 197, loss = 0.35749679\n",
            "Iteration 198, loss = 0.35959964\n",
            "Iteration 199, loss = 0.35927319\n",
            "Iteration 200, loss = 0.36119063\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.96      0.90       980\n",
            "           1       0.95      0.97      0.96      1135\n",
            "           2       0.91      0.90      0.90      1032\n",
            "           3       0.92      0.88      0.90      1010\n",
            "           4       0.92      0.88      0.90       982\n",
            "           5       0.91      0.81      0.86       892\n",
            "           6       0.92      0.94      0.93       958\n",
            "           7       0.89      0.87      0.88      1028\n",
            "           8       0.81      0.81      0.81       974\n",
            "           9       0.83      0.88      0.85      1009\n",
            "\n",
            "    accuracy                           0.89     10000\n",
            "   macro avg       0.89      0.89      0.89     10000\n",
            "weighted avg       0.89      0.89      0.89     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(layers.Dense(10, input_dim=784, activation=\"relu\"))\n",
        "model.add(layers.Dense(5, activation=\"relu\"))\n",
        "model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "    metrics=tf.keras.metrics.Accuracy()\n",
        ")\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10, dtype='int32')\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10, dtype='int32')\n",
        "\n",
        "checkpoint_filepath = '/content/drive/MyDrive/Colab Notebooks'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=10, epochs=5, shuffle=True, validation_split=0.3, callbacks=[model_checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3M6O28Qcu-V7",
        "outputId": "a2336422-4b9a-4624-a0af-e2ba348ded5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 10)                7850      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5)                 55        \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                60        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,965\n",
            "Trainable params: 7,965\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "4200/4200 [==============================] - 10s 2ms/step - loss: 2.1515 - accuracy: 0.0060 - val_loss: 1.7125 - val_accuracy: 0.0132\n",
            "Epoch 2/5\n",
            "4200/4200 [==============================] - 9s 2ms/step - loss: 1.6516 - accuracy: 0.0060 - val_loss: 1.4835 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/5\n",
            "4200/4200 [==============================] - 9s 2ms/step - loss: 1.3858 - accuracy: 0.0000e+00 - val_loss: 1.2564 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/5\n",
            "4200/4200 [==============================] - 9s 2ms/step - loss: 1.2510 - accuracy: 0.0000e+00 - val_loss: 1.2386 - val_accuracy: 5.5556e-06\n",
            "Epoch 5/5\n",
            "4200/4200 [==============================] - 9s 2ms/step - loss: 1.2239 - accuracy: 2.3810e-06 - val_loss: 1.2175 - val_accuracy: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6ddf4b9450>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = clf.predict(x_test)\n",
        "y_test = np.argmax(y_test, axis=1)\n",
        "print(classification_report(y_test, pred))"
      ],
      "metadata": {
        "id": "wkrWFLXa2jsy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee5b683f-d6d2-411e-b5e5-50c67b5d1ec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.96      0.90       980\n",
            "           1       0.95      0.97      0.96      1135\n",
            "           2       0.91      0.90      0.90      1032\n",
            "           3       0.92      0.88      0.90      1010\n",
            "           4       0.92      0.88      0.90       982\n",
            "           5       0.91      0.81      0.86       892\n",
            "           6       0.92      0.94      0.93       958\n",
            "           7       0.89      0.87      0.88      1028\n",
            "           8       0.81      0.81      0.81       974\n",
            "           9       0.83      0.88      0.85      1009\n",
            "\n",
            "    accuracy                           0.89     10000\n",
            "   macro avg       0.89      0.89      0.89     10000\n",
            "weighted avg       0.89      0.89      0.89     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(checkpoint_filepath)"
      ],
      "metadata": {
        "id": "sbDL-9_039Zj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2b13155-8719-43b4-b2ef-60d100867285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f6dddae3950>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}